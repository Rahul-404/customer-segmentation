{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949c4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed45bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/customer-segmentation/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f1964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff920b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/customer-segmentation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fc228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pandas import DataFrame\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from customer_segmentation.logger import logging\n",
    "from customer_segmentation.exception import CustomerException\n",
    "\n",
    "from customer_segmentation.constant.database import DATABASE_NAME, COLLECTION_NAME\n",
    "\n",
    "# from customer_segmentation.entity.config_entiry import DataIngestionConfig\n",
    "\n",
    "# from customer_segmentation.entity.artifact_entity import DataIngestionArtifact\n",
    "\n",
    "from customer_segmentation.data_access.customer_data import CustomerData\n",
    "\n",
    "from customer_segmentation.utils.main_utils import MainUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9d008",
   "metadata": {},
   "source": [
    "### config_entity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83a76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from customer_segmentation.constant.trainin_pipeline import *\n",
    "\n",
    "TIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b25e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingPieplineConfig:\n",
    "    pipeline_name: str = PIPELINE_NAME\n",
    "    artifact_dir: str = os.path.join(PIPELINE_NAME, ARTIFACT_DIR, TIMESTAMP)\n",
    "    timestamp: str = TIMESTAMP\n",
    "\n",
    "training_pipeline_config: TrainingPieplineConfig = TrainingPieplineConfig()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    data_ingestion_dir: str = os.path.join(training_pipeline_config.artifact_dir,\n",
    "                                           DATA_INGESTION_DIR_NAME)\n",
    "    feature_store_file_path: str = os.path.join(data_ingestion_dir,\n",
    "                                                DATA_INGESTION_FEATURE_STORE_DIR,\n",
    "                                                FILE_NAME)\n",
    "    ingested_data_dir: str = os.path.join(data_ingestion_dir,\n",
    "                                          DATA_INGESTION_INGESTED_DIR)\n",
    "    training_file_path: str = os.path.join(data_ingestion_dir,\n",
    "                                           DATA_INGESTION_INGESTED_DIR, \n",
    "                                           TRAIN_FILE_NAME)\n",
    "    testing_file_path: str = os.path.join(data_ingestion_dir,\n",
    "                                          DATA_INGESTION_INGESTED_DIR,\n",
    "                                          TEST_FILE_NAME)\n",
    "    train_test_split_ratio: float = DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO\n",
    "    collection_name: str = DATA_INGESTION_COLLECTION_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2991d",
   "metadata": {},
   "source": [
    "### artifact_entity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7598840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a80848",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    train_file_path: str\n",
    "    test_file_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444b56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataIngestion:\n",
    "\n",
    "    def __init__(self, data_ingestion_config: DataIngestionConfig = DataIngestionConfig()):\n",
    "        self.data_ingestion_config = data_ingestion_config\n",
    "        self.utils = MainUtils()\n",
    "\n",
    "    def export_data_into_feature_store(self)->DataFrame:\n",
    "        \"\"\"\n",
    "        Method Name : export_data_into_feature_store\n",
    "        Description : This method reads data from mongodb and saves it into artifacts.\n",
    "\n",
    "        Output      : dataset is returned as a DataFrame\n",
    "        On Failure  : Write an exception log and then raise an exception\n",
    "\n",
    "        Version     : 0.1 \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Exporting data from mongodb\")\n",
    "            # customer data object\n",
    "            customer_data = CustomerData()\n",
    "            # using object fetching data from mongodb give a collection name\n",
    "            customer_dataframe = customer_data.export_data_collection_as_dataframe(\n",
    "                collection_name = COLLECTION_NAME\n",
    "            )\n",
    "            # logging the shape of data\n",
    "            logging.info(f\"Shape of dataframe: {customer_dataframe.shape}\")\n",
    "            # preparing path to store data\n",
    "            feature_store_file_path = self.data_ingestion_config.feature_store_file_path\n",
    "            # converting it to be a directory\n",
    "            dir_path = os.path.dirname(feature_store_file_path)\n",
    "            # creating the directory to store data if not exists\n",
    "            os.makedirs(dir_path, exist_ok= True)\n",
    "            # logging the store data path\n",
    "            logging.info(f\"Saving exported data into feature store file path : {feature_store_file_path}\")\n",
    "            # saving the fetched dataset\n",
    "            customer_dataframe.to_csv(feature_store_file_path, index=False, header=True)\n",
    "\n",
    "            return customer_dataframe\n",
    "        except Exception as e:\n",
    "            raise CustomerException(e, sys)\n",
    "        \n",
    "    def split_data_as_train_test(self, dataframe: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "        \"\"\"\n",
    "        Method Name : split_data_as_train_test\n",
    "        Descrption  : This method splits the dataframe into train set and test set based on split ratio\n",
    "\n",
    "        Output      : Folder is created in s3 bucket\n",
    "        On Failure  : Write an expectation log and then raise an exception\n",
    "\n",
    "        Version     : 0.1\n",
    "        \"\"\"\n",
    "        logging.info(\"Entered split_data_as_train_test method of DataIngestion class\")\n",
    "        try:\n",
    "            # splitting data into train test \n",
    "            train_set, test_set = train_test_split(dataframe, test_size=self.data_ingestion_config.train_test_split_ratio)\n",
    "            logging.info(\"Exited split_data_as_train_test method of DataIngestion class\")\n",
    "            # path to store train test data\n",
    "            ingested_data_dir = self.data_ingestion_config.ingested_data_dir\n",
    "            # creating directory\n",
    "            os.makedirs(ingested_data_dir, exist_ok=True)\n",
    "            # saving training data\n",
    "            train_set.to_csv(self.data_ingestion_config.training_file_path, index=False, header=True)\n",
    "            logging.info(\"Training data has been saved\")\n",
    "            # saving testing data\n",
    "            test_set.to_csv(self.data_ingestion_config.testing_file_path, index=False, header=True)\n",
    "            logging.info(\"Testing data has been saved\")\n",
    "        except Exception as e:\n",
    "            raise CustomerException(e, sys)\n",
    "\n",
    "    def initiate_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        \"\"\"\n",
    "        Method Name : initialize_data_ingestion\n",
    "        Description : This method initiated the data ingestion components of training pipeline\n",
    "\n",
    "        Output      : train set and test set are returned as the artifacts of data ingestion components\n",
    "        On Failure  : write an exception log and then raise an execution\n",
    "\n",
    "        Version     : 1.0\n",
    "        Revisions   : moved setup to closed \n",
    "        \"\"\"\n",
    "        logging.info(\"Entered initiate_data_ingestion method of DataIngestion class\")\n",
    "\n",
    "        try:\n",
    "            dataframe = self.export_data_into_feature_store()\n",
    "\n",
    "            _schema_config = self.utils.read_schema_config_file()\n",
    "\n",
    "            dataframe = dataframe.drop(_schema_config[\"drop_columns\"], axis=1)\n",
    "\n",
    "            logging.info(\"Got the data from mongodb\")\n",
    "\n",
    "            self.split_data_as_train_test(dataframe)\n",
    "\n",
    "            logging.info(\"Exited initiate_data_ingestion method og DataIngestion class\")\n",
    "\n",
    "            data_ingestion_artifact = DataIngestionArtifact(\n",
    "                train_file_path=self.data_ingestion_config.training_file_path,\n",
    "                test_file_path=self.data_ingestion_config.testing_file_path\n",
    "            )\n",
    "            logging.info(f\"Data ingestion artifact: {data_ingestion_artifact}\")\n",
    "\n",
    "            return data_ingestion_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomerException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add2dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/customer-segmentation\n",
      "DataIngestionArtifact(train_file_path='customer_segmentation/artifact/05_05_2025_15_22_59/data_ingestion/ingested/train.csv', test_file_path='customer_segmentation/artifact/05_05_2025_15_22_59/data_ingestion/ingested/test.csv')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(os.getcwd())\n",
    "    data_ingestion_object = DataIngestion()\n",
    "    data_ingestion_artifact = data_ingestion_object.initiate_data_ingestion()\n",
    "    print(data_ingestion_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf496f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
